{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terrafying/DL-Art-School/blob/master/Copy_of_tortoise_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Usage guide available [here](https://github.com/152334H/DL-Art-School/blob/master/COLAB_USAGE.md)"
      ],
      "metadata": {
        "id": "GVW2hTeg2k5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check NVIDIA GPU"
      ],
      "metadata": {
        "id": "UjvK4JWKdYqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Z6kCeDNrdWM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al /content/DL-Art-School/experiments"
      ],
      "metadata": {
        "id": "CKoP9KPQT13-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive (To save trained checkpoints and to load the dataset from)"
      ],
      "metadata": {
        "id": "Agpune8JjsQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "xFt4umCqjlS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install requirements."
      ],
      "metadata": {
        "id": "sr_N55tZjFWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqAlGZg0hAw3"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!if ! test -d DL-Art-School; then git clone https://github.com/terrafying/DL-Art-School; else cd DL-Art-School; git pull; fi\n",
        "\n",
        "%cd /content/DL-Art-School\n",
        "!wget https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth -O experiments/dvae.pth\n",
        "!wget https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth -O experiments/autoregressive.pth\n",
        "\n",
        "!pip install -r codes/requirements.laxed.txt\n",
        "# !pip install -r codes/requirements_frozen_only_use_if_something_broken.txt\n",
        "#Seems that the project doesn't train without tensorboard ??.\n",
        "#!pip uninstall -y tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VERSION=\"v4.2.0\"\n",
        "BINARY=\"yq_linux_amd64\"\n",
        "!wget https://github.com/mikefarah/yq/releases/download/{VERSION}/{BINARY}.tar.gz -O - |\\\n",
        "  tar xz && mv {BINARY} /usr/bin/yq"
      ],
      "metadata": {
        "id": "nWXuUFmOMOTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Check the integrity of the dVAE checkpoint\n",
        "!sha256sum ../DL-Art-School/experiments/dvae.pth | grep a990825371506c16bcf0e8167bf24ccf82f65bb6a1dbcbfcf058d76f9b197e35 || echo \"SOMETHING IS WRONG WITH THE CHECKPOINT; REPORT THIS AS A GITHUB ISSUE AND DO NOT PROCEED\"\n",
        "# @markdown You should see the following message when verified:\n",
        "\n",
        "# @markdown > `a990825371506c16bcf0e8167bf24ccf82f65bb6a1dbcbfcf058d76f9b197e35  ../DL-Art-School/experiments/dvae.pth`\n"
      ],
      "metadata": {
        "id": "LdYwFcrFMRVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!rm -rf theprimeagen ||:\n",
        "!unzip /content/gdrive/MyDrive/Audio/theprimeagen.zip -x '__MACOSX/*'"
      ],
      "metadata": {
        "id": "hSrZfKA73uIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPEN THIS) Parameters to be able to train on colab."
      ],
      "metadata": {
        "id": "BQTong_qj7sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from math import ceil\n",
        "DEFAULT_TRAIN_BS = 64\n",
        "DEFAULT_VAL_BS = 32\n",
        "#@markdown # Hyperparameter calculation\n",
        "#@markdown Run this cell to obtain suggested parameters for training\n",
        "Dataset_Training_Path = \"/content/theprimeagen/train.csv\" #@param {type:\"string\"}\n",
        "ValidationDataset_Training_Path = \"/content/theprimeagen/val.csv\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### **NOTE**: Dataset must be in the following format.\n",
        "\n",
        "#@markdown  `dataset/`\n",
        "#@markdown * ---├── `val.txt`\n",
        "#@markdown * ---├── `train.txt`\n",
        "#@markdown * ---├── `wavs/`\n",
        "\n",
        "#@markdown `wavs/` directory must contain `.wav` files.\n",
        "\n",
        "#@markdown  Example for `train.txt` and `val.txt`:\n",
        "\n",
        "#@markdown * `wavs/A.wav|Write the transcribed audio here.`\n",
        "\n",
        "#@markdown todo: actually check the dataset structure\n",
        "\n",
        "if Dataset_Training_Path == ValidationDataset_Training_Path:\n",
        "  print(\"WARNING: training dataset path == validation dataset path!!!\")\n",
        "  print(\"\\tThis is technically okay but will make all of the validation metrics useless. \")\n",
        "  print(\"it will also SUBSTANTIALLY slow down the rate of training, because validation datasets are supposed to be much smaller than training ones.\")\n",
        "\n",
        "def txt_file_lines(p: str) -> int:\n",
        "  return len(Path(p).read_text().strip().split('\\n'))\n",
        "training_samples = txt_file_lines(Dataset_Training_Path)\n",
        "val_samples = txt_file_lines(ValidationDataset_Training_Path)\n",
        "\n",
        "if training_samples < 128: print(\"WARNING: very small dataset! the smallest dataset tested thus far had ~200 samples.\")\n",
        "if val_samples < 20: print(\"WARNING: very small validation dataset! val batch size will be scaled down to account\")\n",
        "\n",
        "def div_spillover(n: int, bs: int) -> int: # returns new batch size\n",
        "  epoch_steps,remain = divmod(n,bs)\n",
        "  if epoch_steps*2 > bs: return bs # don't bother optimising this stuff if epoch_steps are high\n",
        "  if not remain: return bs # unlikely but still\n",
        "\n",
        "  if remain*2 < bs: # \"easier\" to get rid of remainder -- should increase bs\n",
        "    target_bs = n//epoch_steps\n",
        "  else: # easier to increase epoch_steps by 1 -- decrease bs\n",
        "    target_bs = n//(epoch_steps+1)\n",
        "  assert n%target_bs < epoch_steps+2 # should be very few extra \n",
        "  return target_bs\n",
        "\n",
        "if training_samples < DEFAULT_TRAIN_BS:\n",
        "  print(\"WARNING: dataset is smaller than a single batch. This will almost certainly perform poorly. Trying anyway\")\n",
        "  train_bs = training_samples\n",
        "else:\n",
        "  train_bs = div_spillover(training_samples, DEFAULT_TRAIN_BS)\n",
        "if val_samples < DEFAULT_VAL_BS:\n",
        "  val_bs = val_samples\n",
        "else:\n",
        "  val_bs = div_spillover(val_samples, DEFAULT_VAL_BS)\n",
        "\n",
        "steps_per_epoch = training_samples//train_bs\n",
        "lr_decay_epochs = [20, 40, 56, 72]\n",
        "lr_decay_steps = [steps_per_epoch * e for e in lr_decay_epochs]\n",
        "print_freq = min(100, max(20, steps_per_epoch))\n",
        "val_freq = save_checkpoint_freq = print_freq * 3\n",
        "\n",
        "print(\"===CALCULATED SETTINGS===\")\n",
        "print(f'{train_bs=} {val_bs=}')\n",
        "print(f'{val_freq=} {lr_decay_steps=}')\n",
        "print(f'{print_freq=} {save_checkpoint_freq=}')"
      ],
      "metadata": {
        "id": "lQQC93cjfmNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##_Settings for normal users:_\n",
        "Experiment_Name = \"PrimeAgen\" #@param {type:\"string\"}\n",
        "Dataset_Training_Name= \"TestPrimeAgenDataset\" #@param {type:\"string\"}\n",
        "ValidationDataset_Name = \"TestValidation\" # this seems to be useless??? @param {type:\"string\"}\n",
        "SaveTrainingStates = False # @param {type:\"boolean\"}\n",
        "Keep_Last_N_Checkpoints = 1 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "n_workers = 4 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown * **NOTE**: 0 means \"keep all models saved\", which could potentially cause out-of-storage issues.\n",
        "#@markdown * Without training states, each model \"only\" takes up ~1.6GB. You should have ~50GB of free space to begin with.\n",
        "#@markdown * With training states, each model (pth+state) takes up ~4.9 GB; Colab will crash around ~10 undeleted checkpoints in this case.\n",
        "\n",
        "#@markdown ##_Other training parameters_\n",
        "Fp16 = False #@param {type:\"boolean\"}\n",
        "Use8bit = True #@param {type:\"boolean\"}\n",
        "#@markdown * **NOTE**: for some reason, fp16 does not seem to improve vram use when combined with 8bit [citation needed]. To be verified later...\n",
        "TrainingRate = \"1e-5\" #@param {type:\"string\"}\n",
        "TortoiseCompat = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown * **NOTE**: TortoiseCompat introduces some breaking changes to the training process. **If you want to reproduce older models**, disable this checkbox.\n",
        "\n",
        "#@markdown ##_Calculated settings_ override\n",
        "#@markdown #####Blank entries rely on the calculated defaults from the cell above.\n",
        "#@markdown ######**Leave them blank unless you want to adjust them manually**\n",
        "TrainBS = \"\" #@param {type:\"string\"}\n",
        "ValBS = \"\" #@param {type:\"string\"}\n",
        "ValFreq = \"\" #@param {type:\"string\"}\n",
        "LRDecaySteps = \"\" #@param {type:\"string\"}\n",
        "PrintFreq = \"\" #@param {type:\"string\"}\n",
        "SaveCheckpointFreq = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def take(orig, override):\n",
        "  if override == \"\": return orig\n",
        "  return type(orig)(override)\n",
        "\n",
        "train_bs = take(train_bs, TrainBS)\n",
        "val_bs = take(val_bs, ValBS)\n",
        "val_freq = take(val_freq, ValFreq)\n",
        "lr_decay_steps = eval(LRDecaySteps) if LRDecaySteps else lr_decay_steps\n",
        "print_freq = take(print_freq, PrintFreq)\n",
        "save_checkpoint_freq = take(save_checkpoint_freq, SaveCheckpointFreq)\n",
        "assert len(lr_decay_steps) == 4 \n",
        "gen_lr_steps = ','.join(['\"{}\"'.format(item) for item in lr_decay_steps])\n",
        "\n",
        "#@markdown #Run this cell after you finish editing the settings.\n",
        "\n",
        "\n",
        "%cd /content/DL-Art-School\n",
        "!wget https://raw.githubusercontent.com/152334H/DL-Art-School/master/experiments/EXAMPLE_gpt.yml -O experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "#@markdown This will apply the settings defined above to a fresh yml config file.\n",
        "import os\n",
        "%cd /content/DL-Art-School\n",
        "import yaml\n",
        "\n",
        "# example_yaml = yaml.safe_load_all(open('./experiments/EXAMPLE_gpt.yml'))\n",
        "!yq e '.datasets.train.n_workers = '$n_workers'' -i ./experiments/EXAMPLE_gpt.yml\n",
        "!yq e '.datasets.train.batch_size = '$train_bs'' -i ./experiments/EXAMPLE_gpt.yml\n",
        "!yq e '.datasets.val.batch_size = '$val_bs'' -i ./experiments/EXAMPLE_gpt.yml\n",
        "!yq e '.datasets.val.batch_size = '$val_bs'' -i ./experiments/EXAMPLE_gpt.yml\n",
        "!yq e '.train.gen_lr_steps = [{gen_lr_steps}]' -i ./experiments/EXAMPLE_gpt.yml\n",
        "# lr_steps_str=\n",
        "\n",
        "!sed -i 's/print_freq: 100/print_freq: '\"$print_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/save_checkpoint_freq: 500/save_checkpoint_freq: '\"$save_checkpoint_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "!sed -i 's+CHANGEME_validation_dataset_name+'\"$ValidationDataset_Name\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's+CHANGEME_path_to_validation_dataset+'\"$ValidationDataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "if(Fp16==True):\n",
        "  os.system(\"sed -i 's+fp16: false+fp16: true+g' ./experiments/EXAMPLE_gpt.yml\")\n",
        "!sed -i 's/use_8bit: true/use_8bit: '\"$Use8bit\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "!sed -i 's/disable_state_saving: true/disable_state_saving: '\"$SaveTrainingStates\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/tortoise_compat: True/tortoise_compat: '\"$TortoiseCompat\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/number_of_checkpoints_to_save: 0/number_of_checkpoints_to_save: '\"$Keep_Last_N_Checkpoints\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "\n",
        "!sed -i 's/CHANGEME_training_dataset_name/'\"$Dataset_Training_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/CHANGEME_your_experiment_name/'\"$Experiment_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's+CHANGEME_path_to_training_dataset+'\"$Dataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "\n",
        "if (not TrainingRate==\"1e-5\"):\n",
        "  os.system(\"sed -i 's+!!float 1e-5 # CHANGEME:+!!float '\" + TrainingRate + \"' #+g' ./experiments/EXAMPLE_gpt.yml\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ryVeEXfxqw3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt"
      ],
      "metadata": {
        "id": "EG9-M55US1us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.19.0"
      ],
      "metadata": {
        "id": "BrxFzHSNUZ2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.util.hub import move_cache\n",
        "\n",
        "move_cache()"
      ],
      "metadata": {
        "id": "95-6x4k8UzTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Ay0tYbU8DUTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Press the stop button for this cell when you are satisfied with the results, and have seen:\n",
        "\n",
        "#@markdown `INFO:base:Saving models and training states.`\n",
        "\n",
        "#@markdown If your training run saves many models, you might exceed the storage limits on the colab runtime. To prevent this, try to delete old checkpoints in /content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/* via the file explorer panel as the training runs. **Resuming training after a crash requires config editing,** so try to not let that happen.\n",
        "\n",
        "#@markdown TODO: implement code to automatically prune useless checkpoints later && restore training states\n",
        "\n",
        "!cd /content/DL-Art-School/codes && \\\n",
        "  python3 train.py -opt ../experiments/EXAMPLE_gpt.yml"
      ],
      "metadata": {
        "id": "ju5yCN_m51r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export to Google Drive"
      ],
      "metadata": {
        "id": "iyGVnx5jrnvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/\"):\n",
        "  print(\"Connect your Google Drive and try again.\")\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/tortoise\"):\n",
        "  os.mkdir(\"/content/gdrive/MyDrive/tortoise\")\n",
        "\n",
        "srcdir = '/content/DL-Art-School/experiments/'+Experiment_Name\n",
        "outdir = '/content/gdrive/MyDrive/tortoise/'+Experiment_Name\n",
        "if not os.path.exists(outdir):\n",
        "  os.mkdir(outdir)\n",
        "\n",
        "from pathlib import Path\n",
        "from shutil import copy, copytree\n",
        "outdir = Path(outdir)\n",
        "srcdir = Path(srcdir)\n",
        "\n",
        "#@markdown #Pick what training results you'd like to save:\n",
        "#deleteZeroStepCheckpoints = True #@param {type:\"boolean\"}\n",
        "saveLogs = True #@param {type:\"boolean\"}\n",
        "saveYml = True #@param {type:\"boolean\"}\n",
        "saveInferenceCheckpoints = True #@param {type:\"boolean\"}\n",
        "#@markdown These only need to be adjusted if you want to commit further training:\n",
        "saveEMACheckpoints = False #@param {type:\"boolean\"}\n",
        "saveTrainingStates = False #@param {type:\"boolean\"}\n",
        "\n",
        "checkpointSavingStrategy = 'minimal (last ckpt only)' #@param [\"Everything (I have infinite storage)\", \"minimal (last ckpt only)\"]\n",
        "#@markdown Note that each checkpoint takes up **1.6GB of space** -- if you want to save all checkpoints, you probably need more than 15GB of gdrive storage.\n",
        "\n",
        "\n",
        "outdir.mkdir(exist_ok=True)\n",
        "'''\n",
        "if deleteZeroStepCheckpoints:\n",
        "    for zero_model in srcdir.glob('models/0_gpt*.pth'):\n",
        "        zero_model.unlink() # remove all 0 step files; useless\n",
        "    for zero_model in srcdir.glob('training_state/0.state'):\n",
        "        zero_model.unlink()\n",
        "'''\n",
        "if saveLogs:\n",
        "    copytree(srcdir/'tb_logger', outdir/'tb_logger')\n",
        "    for log in srcdir.glob('*.log'):\n",
        "        copy(log, outdir)\n",
        "\n",
        "if saveYml:\n",
        "    for yml in srcdir.glob('*.yml'):\n",
        "        copy(yml, outdir)\n",
        "\n",
        "infer_models = list((srcdir/'models').glob('*_gpt.pth'))\n",
        "training_states = (srcdir/'training_state').iterdir()\n",
        "ema_models = (srcdir/'models').glob('*_gpt_ema.pth')\n",
        "highest_step = max(int(m.stem.split('_')[0]) for m in infer_models)\n",
        "\n",
        "def save_model_directory(glob, outsubdir):\n",
        "    outsubdir.mkdir(exist_ok=True)\n",
        "    for m in glob:\n",
        "        if checkpointSavingStrategy != 'minimal (last ckpt only)' \\\n",
        "            or int(m.stem.split('_')[0]) == highest_step:\n",
        "            copy(m, outsubdir)\n",
        "\n",
        "if saveInferenceCheckpoints:\n",
        "    save_model_directory(infer_models, outdir/'models')\n",
        "\n",
        "if saveEMACheckpoints:\n",
        "    save_model_directory(ema_models, outdir/'models')\n",
        "\n",
        "if saveTrainingStates:\n",
        "    save_model_directory(training_states, outdir/'training_state')\n",
        "\n"
      ],
      "metadata": {
        "id": "049dUor8rlxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ###If the above script does not work, use this old cell\n",
        "\n",
        "#@markdown #This will blindly copy everything to your gdrive folder.\n",
        "\n",
        "!cp -r /content/DL-Art-School/experiments/$Experiment_Name /content/gdrive/MyDrive/tortoise/"
      ],
      "metadata": {
        "id": "BqydhU4Gwlv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}